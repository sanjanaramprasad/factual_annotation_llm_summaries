{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7591a1e-c178-4346-a85b-f4b89d6d3b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c09bd5cc67498a8b498eb219e1f723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e595edf333e4e5d8b2700a8058c80ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c030b1462d2a4292864f4997e1fab74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset billsum/default to /scratch/ramprasad.sa/huggingface_datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b47f761f114283b3b93e6fdb4e4f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/67.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/18949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ca_test split:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset billsum downloaded and prepared to /scratch/ramprasad.sa/huggingface_datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"billsum\", split=\"test\", cache_dir = '/scratch/ramprasad.sa/huggingface_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9fe0111-335e-48dc-ba28-a99c93807f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import dataset_creators.config as config\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "# import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38512cc4-bb03-4486-ad7b-56e4d9c0aca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config.instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3820532-d0fc-45ef-a02b-a4e79cc6d052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_prompt_token_limits(article, instructions, tokenizer, token_limit):\n",
    "    counter = 0\n",
    "    for key, instruction in instructions.items():\n",
    "        prompt = f'Article: {article}\\n{instruction}'\n",
    "        prompt_len = len(tokenizer.encode(prompt))\n",
    "        if prompt_len < token_limit:\n",
    "            counter += 1 \n",
    "    return counter \n",
    "\n",
    "def get_shortlisted_data(articles, reference_summaries, ids = [], token_limit = 4096, dataset = 'xsum'):\n",
    "    \n",
    "    flan_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "    flan_instructions = config.instructions[f'{dataset}_flant5']\n",
    "    \n",
    "    gpt_tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    gpt_instructions = config.instructions[f'{dataset}_gpt3']\n",
    "    \n",
    "    shortlisted_articles = []\n",
    "    shortlisted_reference_summaries = []\n",
    "    \n",
    "    shortlisted_ids = []\n",
    "    \n",
    "    for idx, article in enumerate(tqdm(articles)):\n",
    "        add_article = 0 \n",
    "        \n",
    "        flan_counter = check_prompt_token_limits(article, flan_instructions, flan_tokenizer, token_limit)\n",
    "        add_article += flan_counter\n",
    "        \n",
    "        gpt_counter = check_prompt_token_limits(article, gpt_instructions, gpt_tokenizer, token_limit)\n",
    "        add_article += gpt_counter\n",
    "        \n",
    "\n",
    "        if add_article == 4:\n",
    "            shortlisted_articles.append(article)\n",
    "            shortlisted_reference_summaries.append(reference_summaries[idx])\n",
    "            \n",
    "            if not ids:\n",
    "                article_id = str(uuid.uuid4())\n",
    "            else:\n",
    "                article_id = ids[idx]\n",
    "            \n",
    "            shortlisted_ids.append(article_id)\n",
    "            \n",
    "    return shortlisted_articles, shortlisted_reference_summaries, shortlisted_ids\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee5faff-5ed6-48c5-aec0-705fb22d9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_billsum(data_path, token_limit = 4096):\n",
    "    dataset = load_dataset(\"billsum\", split=\"test\", cache_dir = '/scratch/ramprasad.sa/huggingface_datasets')\n",
    "    articles = dataset['text']\n",
    "    reference_summaries = dataset['summary']\n",
    "    \n",
    "    \n",
    "    \n",
    "    shortlisted_data = {'article': [], 'reference_summary': [], 'id': [], 'origin': []}\n",
    "    \n",
    "    shortlisted_articles, shortlisted_reference_summaries, shortlisted_ids = get_shortlisted_data(articles, reference_summaries, dataset = 'billsum')\n",
    "    shortlisted_data['article'] += shortlisted_articles\n",
    "    shortlisted_data['reference_summary'] += shortlisted_reference_summaries\n",
    "    shortlisted_data['id'] += shortlisted_ids\n",
    "    shortlisted_data['origin'] += ['billsum'] * len(shortlisted_ids)\n",
    "            \n",
    "    isExist = os.path.exists(data_path)\n",
    "    if not isExist:\n",
    "        os.makedirs(data_path)\n",
    "    \n",
    "    df = pd.DataFrame(shortlisted_data)\n",
    "    df.to_csv(f'{data_path}/test_sample.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54525258-7e12-4488-8792-e19ed8c9e93c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset billsum (/scratch/ramprasad.sa/huggingface_datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc)\n",
      "  0%|                                                                                                                                                                                        | 0/3269 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1820 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3269/3269 [01:04<00:00, 50.55it/s]\n"
     ]
    }
   ],
   "source": [
    "df_result = make_sample_billsum('/home/ramprasad.sa/factual_annotation_llm_summaries/datasets/billsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ded421e-e531-4557-9187-f5c8b7e16a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amends the Water Resources Development Act of 1999 to: (1) authorize appropriations for FY 1999 through 2009 for implementation of a long-term resource monitoring program with respect to the Upper Mississippi River Environmental Management Program (currently, such funding is designated for a program for the planning, construction, and evaluation of measures for fish and wildlife habitat rehabilitation and enhancement); (2) authorize the Secretary of the Army to carry out modifications to the navigation project for the Delaware River, Pennsylvania and Delaware, if such project as modified is technically sound, environmentally (currently, economically) acceptable, and economically justified; (3) subject certain previously deauthorized water resources development projects to the seven-year limitation governing project deauthorizations under the Act, with the exception of such a project for Indian River County, Florida; (4) except from a certain schedule of the non-Federal cost of the periodic nourishment of shore protection projects constructed after December 31, 1999, those projects for which a District Engineer's Report has been completed by such date;  (5) require that the project cooperation agreement for the Comite River Diversion Project for flood control include a provision that specifies that any reduction in the non-Federal share that results from certain modifications be credited toward the share of project costs to be paid by the Amite River Basin Drainage and Water Conservation District; (6) allow the Secretary to provide additional compensation to Chesapeake City, Maryland (currently, to the City of Chesapeake, Maryland) for damage to its water supply resulting from the Chesapeake and Delaware Canal Project; (7) provide for the submission of certain reports on water resources development projects by the Secretary, notwithstanding Federal reporting termination provisions; and (8) authorize and provide for an authorization of appropriations for the existing program for the safety and operations expenses of the Federal Railroad Administration, and make available for obligation funds currently appropriated for such program.\n"
     ]
    }
   ],
   "source": [
    "print(list(df_result['reference_summary'].values)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac90d8-9ceb-40b0-af2e-646b99a8e556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
