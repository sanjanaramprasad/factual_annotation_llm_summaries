{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13befce-628a-47ac-835b-3fa9710841f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math \n",
    "import string \n",
    "import re\n",
    "\n",
    "import json \n",
    "\n",
    "import pandas as pd \n",
    "import sqlite3\n",
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719896e2-78cf-4e50-8892-85c3fc764f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qualified_annotators_dict = {'pubmed': ['ann_jclvzw', 'ann_eftpco'],\n",
    "                            'news': ['ann_japq', 'ann_tpfo'],\n",
    "                            'billsum': ['ann_krcnbm', 'ann_hguilf']}\n",
    "\n",
    "\n",
    "def connect_to_db(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    return conn, c \n",
    "\n",
    "\n",
    "def read_annotations(db_path, task):\n",
    "    # /human_annotations_factuality/XSUM_CNN\n",
    "    parent_path = '/home/ramprasad.sa'\n",
    "    db_path_round1 = f'{parent_path}/{db_path}/set1/annotated/{task}_summaries_set1_final.db'\n",
    "    print(db_path_round1)\n",
    "    conn, c = connect_to_db(db_path_round1)\n",
    "    df_annotations_round1  = pd.read_sql('SELECT * from label', conn)\n",
    "\n",
    "    db_path_round2 = f'{parent_path}/{db_path}/set1_round2/annotated/{task}_nonfactual_annotated_generated_summaries_fin.db'\n",
    "    conn, c = connect_to_db(db_path_round2)\n",
    "    df_annotations_round2 = pd.read_sql('SELECT * from error_label', conn)\n",
    "    return df_annotations_round1, df_annotations_round2\n",
    "    \n",
    "    \n",
    "def remove_duplicates(df, task):\n",
    "    \n",
    "    qualified_annotators_dict = {'pubmed': ['ann_jclvzw', 'ann_eftpco'],\n",
    "                            'news': ['ann_japq', 'ann_tpfo'],\n",
    "                            'billsum': ['ann_krcnbm', 'ann_hguilf']}\n",
    "    qualified_annotators = qualified_annotators_dict[task]\n",
    "    processed_rows = []\n",
    "    \n",
    "    for annotator in qualified_annotators:\n",
    "        df_qualified_annotators = df[df['user_id'] == annotator]\n",
    "        # print(df_qualified_annotators)\n",
    "        for each_id in list(set(df_qualified_annotators['summary_uuid'].values)):\n",
    "            df_uid = df_qualified_annotators[df_qualified_annotators['summary_uuid'] == each_id]\n",
    "            if len(df_uid) == 1:\n",
    "                row_append = df_uid.iloc[[0]]\n",
    "            else:\n",
    "                row_append = df_uid.iloc[[-1]]\n",
    "            processed_rows.append(row_append)\n",
    "            \n",
    "    df_processed = pd.concat(processed_rows)\n",
    "    return df_processed\n",
    "\n",
    "def make_pids(df_annotations):\n",
    "    pids = []\n",
    "    for idx, row in df_annotations.iterrows():\n",
    "        summary_uuid = row['summary_uuid'].split('_ann_')[0].strip()\n",
    "        pids.append(summary_uuid)\n",
    "    df_annotations['pid'] = pids\n",
    "    return df_annotations\n",
    "\n",
    "def refine_df(df_annotations, task):\n",
    "    \n",
    "    df_annotations = remove_duplicates(df_annotations, task)\n",
    "    df_annotations = make_pids(df_annotations)\n",
    "    return df_annotations\n",
    "\n",
    "def test_annotation_rounds(df_annotations_round1, df_annotations_round2, task):\n",
    "    \n",
    "    \n",
    "    for qual_ann in qualified_annotators_dict[task]:\n",
    "        df_ann_r1 = df_annotations_round1[df_annotations_round1['user_id'] == qual_ann]\n",
    "        df_ann_r1_sents = df_ann_r1[df_ann_r1['label_type'] == 'non_factual']['nonfactual_sentences'].values\n",
    "        df_ann_r1_sents = [len(each.split('<new_annotation>')) for each in df_ann_r1_sents ]\n",
    "        \n",
    "        df_ann_r2 = df_annotations_round2[df_annotations_round2['user_id'] == qual_ann]\n",
    "        assert sum(df_ann_r1_sents) ==  len(df_ann_r2)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def make_sentence_error_maps(task_annotators,  df_round2_article_summ):\n",
    "    sentence_error_category_map = {}\n",
    "    if not len(df_round2_article_summ):\n",
    "            sentence_error_category_map[None] = {0: (None, None, None, None), 1: (None, None, None, None)}\n",
    "            \n",
    "    else:\n",
    "        for annotator in task_annotators:\n",
    "            df_round2_article_summ_annotator = df_round2_article_summ[df_round2_article_summ['user_id'] == annotator]\n",
    "            for error_idx, error_row in df_round2_article_summ_annotator.iterrows():\n",
    "                    user_id = task_annotators.index(error_row['user_id'])\n",
    "                    nonfactual_sentence = error_row['nonfactual_sentence']\n",
    "                    inaccuracy_severity = error_row['inaccuracy_severity']\n",
    "                    error_type = error_row['error_type']\n",
    "                    error_factuality = error_row['error_factuality'] \n",
    "                    error_factuality = error_factuality if error_factuality else None\n",
    "                    comments = error_row['comments']\n",
    "                    comments = comments if comments else None\n",
    "\n",
    "                    if nonfactual_sentence not in sentence_error_category_map:\n",
    "                        sentence_error_category_map[nonfactual_sentence] = {0: (None, None, None, None), 1: (None, None, None, None)}\n",
    "                    sentence_error_category_map[nonfactual_sentence][user_id] = (inaccuracy_severity, error_type, error_factuality, comments)\n",
    "    return sentence_error_category_map\n",
    "    \n",
    "\n",
    "\n",
    "def rectified_annotations(df_aggr, ann_idx):\n",
    "    \n",
    "    label_types_ann = list(df_aggr[f'label_type_ann{ann_idx}'].values)\n",
    "    inaccuracy_severity_ann = list(df_aggr[f'inaccuracy_severity_ann{ann_idx}'].values)\n",
    "    error_type_ann = list(df_aggr[f'error_type_ann{ann_idx}'].values)\n",
    "    error_factuality_ann = list(df_aggr[f'error_factuality_ann{ann_idx}'].values)\n",
    "    \n",
    "    for idx, row in df_aggr.iterrows():\n",
    "        sumid = row['summary_uuid']\n",
    "        comment = row[f'comments_ann{ann_idx}']\n",
    "        error_type = row[f'error_type_ann{ann_idx}']\n",
    "        if comment != None :\n",
    "            comment = comment.lower()\n",
    "            if 'accurate' in comment or 'factual' in comment or 'this is correct' in comment or 'misidentified' in comment:\n",
    "                # print(comment, idx, label_types_ann[idx])\n",
    "                rect_ids = list(df_aggr[df_aggr['summary_uuid'] == sumid].index)\n",
    "                for rid in rect_ids:\n",
    "                    label_types_ann[rid] =  'factual'\n",
    "                    inaccuracy_severity_ann[rid] = None\n",
    "                    error_type_ann[rid] = None\n",
    "                    error_factuality_ann[rid] = None\n",
    "                \n",
    "    df_aggr[f'label_type_ann{ann_idx}'] = label_types_ann\n",
    "    df_aggr[f'inaccuracy_severity_ann{ann_idx}'] = inaccuracy_severity_ann\n",
    "    df_aggr[f'error_type_ann{ann_idx}'] = error_type_ann\n",
    "    df_aggr[f'error_factuality_ann{ann_idx}'] = error_factuality_ann\n",
    "    return df_aggr\n",
    "\n",
    "def make_aggr_df(df_annotations_round1, df_annotations_round2, task):\n",
    "    \n",
    "    df_processed_dict = {\n",
    "    'summary_uuid': [],\n",
    "    'pid': [],\n",
    "    'summary': [],\n",
    "    'article': [],\n",
    "    'summ_id': [],\n",
    "    'system_id': [],\n",
    "    'nonfactual_sentence': [],\n",
    "    'label_type_ann1': [],\n",
    "    'label_type_ann2': [],\n",
    "    'inaccuracy_severity_ann1': [],\n",
    "    'inaccuracy_severity_ann2': [],\n",
    "    'error_type_ann1': [],\n",
    "    'error_type_ann2': [],  \n",
    "    'error_factuality_ann1': [],\n",
    "    'error_factuality_ann2': [],\n",
    "    'comments_ann1': [],\n",
    "    'comments_ann2': [],\n",
    "\n",
    "    }\n",
    "    \n",
    "    unique_articles = list(set(df_annotations_round1['article'].values))\n",
    "    task_annotators = qualified_annotators_dict[task]\n",
    "\n",
    "    for article in unique_articles:\n",
    "        df_round1_article = df_annotations_round1[df_annotations_round1['article'] == article]\n",
    "        df_round2_article = df_annotations_round2[df_annotations_round2['article'] == article]\n",
    "\n",
    "        article_summaries = list(set(df_round1_article['summary'].values))\n",
    "\n",
    "        for article_summary in article_summaries:\n",
    "\n",
    "            df_round1_article_summ = df_round1_article[df_round1_article['summary'] == article_summary]\n",
    "            df_round2_article_summ = df_round2_article[df_round2_article['summary'] == article_summary]\n",
    "            if len(df_round1_article_summ) <2 :\n",
    "                print(article_summary)\n",
    "            else:\n",
    "                label_type_annotations = []\n",
    "                for annotator in task_annotators:\n",
    "                    df_round1_article_summ_annotator = df_round1_article_summ[df_round1_article_summ['user_id'] == annotator]\n",
    "\n",
    "                    label_type_annotations.append(df_round1_article_summ_annotator['label_type'].values[0])\n",
    "\n",
    "                sentence_error_category_map = make_sentence_error_maps(task_annotators,  df_round2_article_summ)\n",
    "\n",
    "\n",
    "\n",
    "                for nonfactual_sentence, nonfactual_sentence_annotations in sentence_error_category_map.items():\n",
    "                    summary_uuid = df_round1_article_summ['summary_uuid'].values[0]\n",
    "                    summ_id = df_round1_article_summ['summ_id'].values[0]\n",
    "                    system_id = df_round1_article_summ['system_id'].values[0]\n",
    "                    pid = list(set(df_round2_article_summ['pid'].values))\n",
    "                    summary = df_round1_article_summ['summary'].values[0]\n",
    "\n",
    "                    df_processed_dict['summary_uuid'].append(summary_uuid)\n",
    "                    df_processed_dict['summ_id'].append(summ_id)\n",
    "                    system_id_append = 'GPT 3.5' if system_id == 'gpt3' else 'Flan T5'\n",
    "                    df_processed_dict['system_id'].append(system_id_append)\n",
    "                    df_processed_dict['pid'].append(pid)\n",
    "                    # df_processed_dict['pid_2'].append(pid)\n",
    "                    df_processed_dict['summary'].append(summary)\n",
    "                    df_processed_dict['article'].append(article)\n",
    "\n",
    "                    df_processed_dict['nonfactual_sentence'].append(nonfactual_sentence)\n",
    "                    for lid, label_annotations in enumerate(label_type_annotations):\n",
    "                        df_processed_dict[f'label_type_ann{lid+1}'] += [label_annotations]\n",
    "\n",
    "                    # ann_vals = {\n",
    "                    for ann_id, ann_vals in nonfactual_sentence_annotations.items():\n",
    "                        ann_id = ann_id + 1\n",
    "                        df_processed_dict[f'inaccuracy_severity_ann{ann_id}'].append(ann_vals[0])\n",
    "                        df_processed_dict[f'error_type_ann{ann_id}'].append(ann_vals[1])\n",
    "                        df_processed_dict[f'error_factuality_ann{ann_id}'].append(ann_vals[2])\n",
    "                        df_processed_dict[f'comments_ann{ann_id}'].append(ann_vals[3])\n",
    "                        \n",
    "    return pd.DataFrame(df_processed_dict)\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "def compute_score(df_aggr, task):\n",
    "\n",
    "    unique_ids = list(set(df_aggr['summary_uuid']))\n",
    "    df_errors_scores = {'summary_uuid': [], 'score': [], 'model': [], 'task': [], 'summary': [], 'article': []}\n",
    "    for model in ['Flan T5', 'GPT 3.5']:\n",
    "        \n",
    "        df_model = df_aggr[df_aggr['system_id'] == model]\n",
    "        unique_ids = list(set(df_model['summary_uuid']))\n",
    "        print(len(unique_ids))\n",
    "        model_sentbased_scores = []\n",
    "        for uuid in unique_ids:\n",
    "\n",
    "            df_uuid = df_model[df_model['summary_uuid'] == uuid]\n",
    "            # print(df_uuid)\n",
    "            # summary = df_uuid['summary'].values[0]\n",
    "            \n",
    "            ann1_scores = []\n",
    "            ann2_scores = []\n",
    "            for idx, row in df_uuid.iterrows():\n",
    "                summary = row['summary']\n",
    "                summ_sents =  list(nlp(summary).sents)\n",
    "                nonfactual_sentence = row['nonfactual_sentence']\n",
    "                label_type_ann1 = row['inaccuracy_severity_ann1']\n",
    "                label_type_ann2 = row['inaccuracy_severity_ann2']\n",
    "                ann1_score = 0\n",
    "                ann2_score = 0\n",
    "                \n",
    "                if label_type_ann1 != None:\n",
    "                    ann1_score = 1\n",
    "                if label_type_ann2 != None:\n",
    "                    ann2_score = 1\n",
    "\n",
    "                ann1_scores.append(ann1_score)\n",
    "                ann2_scores.append(ann2_score)\n",
    "\n",
    "            ann1_score_uuid = sum(ann1_scores)\n",
    "            ann2_score_uuid = sum(ann2_scores)\n",
    "\n",
    "            sentbased_score = np.mean([ann1_score_uuid, ann2_score_uuid])/len(summ_sents)\n",
    "            # print([ann1_score_uuid, ann2_score_uuid], row)\n",
    "            model_sentbased_scores.append(sentbased_score)\n",
    "            df_errors_scores['summary_uuid'].append(uuid) \n",
    "            df_errors_scores['score'].append(sentbased_score)\n",
    "            # model_append = 'GPT-3.5' if model == 'gpt3' else 'Flan-T5-XL'\n",
    "            df_errors_scores['model'].append(model)\n",
    "            df_errors_scores['task'].append(task)\n",
    "            df_errors_scores['summary'].append(summary)\n",
    "            df_errors_scores['article'].append(row['article'])\n",
    "        print(model.upper(), np.mean(model_sentbased_scores))\n",
    "        \n",
    "    return pd.DataFrame(df_errors_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4730a681-89ef-4c4b-80e8-f8224f75e120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_scoring(db_path, task):\n",
    "    df_annotations_round1, df_annotations_round2 = read_annotations(db_path, task)\n",
    "    print(len(df_annotations_round1), len(df_annotations_round2))\n",
    "    df_annotations_round2 = refine_df(df_annotations_round2, task)\n",
    "    df_annotations_round1 = remove_duplicates(df_annotations_round1, task)\n",
    "    print('removed duplicates...', len(df_annotations_round1), len(df_annotations_round2))\n",
    "\n",
    "    test_annotation_rounds(df_annotations_round1, df_annotations_round2, task)\n",
    "    df_aggr = make_aggr_df(df_annotations_round1, df_annotations_round2, task)\n",
    "    df_aggr = rectified_annotations(df_aggr, ann_idx = 1)\n",
    "    df_aggr = rectified_annotations(df_aggr, ann_idx = 2)\n",
    "    # label_aggr = df_aggr[df_aggr['label_type_ann1'] == df_aggr['label_type_ann2']] \n",
    "    \n",
    "    # df_aggr_sent = df_aggr[(~df_aggr['error_type_ann1'].isnull()) | (~df_aggr['error_type_ann2'].isnull())]\n",
    "    df_errors = compute_score(df_aggr, task)\n",
    "    \n",
    "    write_path = '/home/ramprasad.sa/factual_annotation_llm_summaries/annotations'\n",
    "    df_errors.to_csv(f'{write_path}/{task}_annotations_scores.csv')\n",
    "    df_aggr.to_csv(f'{write_path}/{task}_annotations.csv')\n",
    "    return df_errors, df_aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89e65b-92de-4b20-a5bb-803b38465ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ramprasad.sa//human_annotations_factuality/XSUM_CNN/set1/annotated/news_summaries_set1_final.db\n",
      "219 118\n",
      "removed duplicates... 201 114\n",
      "Delhi Dynamos has expressed an interest in signing Leicester City's midfielder, Esteban Cambiasso, for India's I-League. Nigel Pearson desires to keep the 34-year-old, who signed a one-year deal with Leicester last summer, for another season, however, this is dependent on Leicester's status in the Premier League. Leicester is currently in 18th place in the Premier League and faces a critical relegation encounter against Burnley at Turf Moor on Saturday.\n",
      "50\n",
      "FLAN T5 0.27\n",
      "50\n",
      "GPT 3.5 0.18593290043290042\n",
      "/home/ramprasad.sa//human_annotations_factuality/billsum/set1/annotated/billsum_summaries_set1_final.db\n",
      "233 123\n",
      "removed duplicates... 200 117\n",
      "50\n",
      "FLAN T5 0.19\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "db_path_news = '/human_annotations_factuality/XSUM_CNN'\n",
    "task = 'news'\n",
    "df_errors_news, df_aggr_news = run_scoring(db_path_news, 'news')\n",
    "\n",
    "db_path_billsum = '/human_annotations_factuality/billsum'\n",
    "task = 'billsum'\n",
    "df_errors_billsum, df_aggr_billsum = run_scoring(db_path_billsum, task)\n",
    "\n",
    "db_path_pubmed = '/human_annotations_factuality/pubmed'\n",
    "task = 'pubmed'\n",
    "df_errors_pubmed, df_aggr_pubmed= run_scoring(db_path_pubmed, task)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c591b4-0618-4638-842c-dbdac5617a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
