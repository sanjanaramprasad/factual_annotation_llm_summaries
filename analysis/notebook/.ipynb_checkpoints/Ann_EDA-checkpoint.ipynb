{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb11c79-3b00-43f1-9710-e331018ec8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math \n",
    "import string \n",
    "import re\n",
    "\n",
    "import json \n",
    "\n",
    "import pandas as pd \n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6609027b-fc87-4543-a98d-2a9df7c71545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def connect_to_db(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    return conn, c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8eb844d-57ed-43a1-bdb0-3c089c31cf10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parent_path = '/home/ramprasad.sa'\n",
    "db_path = f'{parent_path}/human_annotations_factuality/XSUM_CNN/set1/annotated/news_summaries_set1_final.db'\n",
    "conn, c = connect_to_db(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50f11a88-b981-4e52-83a7-8d5d5c7614df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qualified_annotators_billsum = ['ann_krcnbm', 'ann_hguilf']\n",
    "qualified_annotators_news = ['ann_japq', 'ann_tpfo']\n",
    "qualified_annotators_pubmed = ['ann_jclvzw', 'ann_eftpco']\n",
    "df_annotations = pd.read_sql('SELECT * from label', conn)\n",
    "# df_annotations = df_annotations[df_annotations['user_id'].isin(qualified_annotators_billsum)]\n",
    "\n",
    "# df_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3776fb78-844b-46e9-8c3d-f34e956bf2af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ann_tpfo', 'ann_japq', 'sanjana']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(df_annotations['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b2b0f61-96b8-4d2a-bac7-2f5432e5d00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def remove_duplicates(df, qualified_annotators):\n",
    "    processed_rows = []\n",
    "    \n",
    "    for annotator in qualified_annotators:\n",
    "        \n",
    "        df_qualified_annotators = df[df['user_id'] == annotator]\n",
    "        for each_id in list(set(df_qualified_annotators['summary_uuid'].values)):\n",
    "            df_uid = df_qualified_annotators[df_qualified_annotators['summary_uuid'] == each_id]\n",
    "            if len(df_uid) == 1:\n",
    "                row_append = df_uid.iloc[[0]]\n",
    "            else:\n",
    "                row_append = df_uid.iloc[[-1]]\n",
    "            processed_rows.append(row_append)\n",
    "            \n",
    "    df_processed = pd.concat(processed_rows)\n",
    "    return df_processed\n",
    "                \n",
    "        \n",
    "\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec465b4-8a81-4328-95e2-e941e0abff38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>user_id</th>\n",
       "      <th>summary_uuid</th>\n",
       "      <th>summ_id</th>\n",
       "      <th>system_id</th>\n",
       "      <th>label_type</th>\n",
       "      <th>summary</th>\n",
       "      <th>nonfactual_sentences</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>211</td>\n",
       "      <td>ann_japq</td>\n",
       "      <td>c1de892aefd7aa1c6c0ad980039252b8c41f208b_flant...</td>\n",
       "      <td>cnndm_generic</td>\n",
       "      <td>flant5</td>\n",
       "      <td>factual</td>\n",
       "      <td>Heather Mack, 19, is accused of murdering her ...</td>\n",
       "      <td></td>\n",
       "      <td>The daughter of Chicago socialite Sheila Von W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>189</td>\n",
       "      <td>ann_japq</td>\n",
       "      <td>6662e68fa9d78198ba62bc347bfedd63dd9c6510_flant...</td>\n",
       "      <td>cnndm_generic</td>\n",
       "      <td>flant5</td>\n",
       "      <td>factual</td>\n",
       "      <td>Footage shows one of the men emerging from his...</td>\n",
       "      <td></td>\n",
       "      <td>This is the shocking moment a BMW driver ramme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>205</td>\n",
       "      <td>ann_japq</td>\n",
       "      <td>9cc6721677443a0d3e8c61eeff14cd75f884ad5e_flant...</td>\n",
       "      <td>cnndm_generic</td>\n",
       "      <td>flant5</td>\n",
       "      <td>factual</td>\n",
       "      <td>CCTV footage shows Petri Kurti, 13, murdering ...</td>\n",
       "      <td></td>\n",
       "      <td>A boy of 13 has become one of the UK's younges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>ann_japq</td>\n",
       "      <td>30002988_gpt3_gen</td>\n",
       "      <td>xsum_generic</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>non_factual</td>\n",
       "      <td>The number of children put up for adoption in ...</td>\n",
       "      <td>Whereas there were 1,830 initial decisions to ...</td>\n",
       "      <td>In the three months to June 2014, there were 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>195</td>\n",
       "      <td>ann_japq</td>\n",
       "      <td>7a6310f1ca0afa6b09777e822b516fc1bf71aa34_flant...</td>\n",
       "      <td>cnndm_generic</td>\n",
       "      <td>flant5</td>\n",
       "      <td>factual</td>\n",
       "      <td>Nursultan Nazarbayev has been re-elected as pr...</td>\n",
       "      <td></td>\n",
       "      <td>The leader of Kazakhstan has apologised after ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uuid   user_id                                       summary_uuid   \n",
       "210   211  ann_japq  c1de892aefd7aa1c6c0ad980039252b8c41f208b_flant...  \\\n",
       "188   189  ann_japq  6662e68fa9d78198ba62bc347bfedd63dd9c6510_flant...   \n",
       "204   205  ann_japq  9cc6721677443a0d3e8c61eeff14cd75f884ad5e_flant...   \n",
       "20     21  ann_japq                                  30002988_gpt3_gen   \n",
       "194   195  ann_japq  7a6310f1ca0afa6b09777e822b516fc1bf71aa34_flant...   \n",
       "\n",
       "           summ_id system_id   label_type   \n",
       "210  cnndm_generic    flant5      factual  \\\n",
       "188  cnndm_generic    flant5      factual   \n",
       "204  cnndm_generic    flant5      factual   \n",
       "20    xsum_generic      gpt3  non_factual   \n",
       "194  cnndm_generic    flant5      factual   \n",
       "\n",
       "                                               summary   \n",
       "210  Heather Mack, 19, is accused of murdering her ...  \\\n",
       "188  Footage shows one of the men emerging from his...   \n",
       "204  CCTV footage shows Petri Kurti, 13, murdering ...   \n",
       "20   The number of children put up for adoption in ...   \n",
       "194  Nursultan Nazarbayev has been re-elected as pr...   \n",
       "\n",
       "                                  nonfactual_sentences   \n",
       "210                                                     \\\n",
       "188                                                      \n",
       "204                                                      \n",
       "20   Whereas there were 1,830 initial decisions to ...   \n",
       "194                                                      \n",
       "\n",
       "                                               article  \n",
       "210  The daughter of Chicago socialite Sheila Von W...  \n",
       "188  This is the shocking moment a BMW driver ramme...  \n",
       "204  A boy of 13 has become one of the UK's younges...  \n",
       "20   In the three months to June 2014, there were 9...  \n",
       "194  The leader of Kazakhstan has apologised after ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = remove_duplicates(df_annotations, qualified_annotators_news)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b3baf5d-11ed-4738-b2f5-4e9a724ea6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['user_id'] == 'ann_krcnbm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e55db073-2cd2-4cc5-ac14-969cfb6867f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris in the spring is the time for lounging on  blankets in a park while taking in the spectacular views, sipping on local wine and nibbling on a fresh baguette and ripe Camembert. Going from shop to shop to collect all of the essentials for a picnic can be a pain, so a made-to-order delivery service is making it easier for tourists to enjoy the French capital like a local. Paris Picnics saves its customers from the hassle of trawling through the marché by delivering a freshly-prepared lunch to wherever they are in the city. Visitors or locals can select made-to-order picnic baskets filled with wine, baguettes, cheese and crisps . Customers can pick from one of the four picnic options on offer and order online. Le Classique picnic, which costs €55 (£40) for two people, comes with a choice of wine, baguettes, an assortment of cheeses, charcuterie, gourmet crisps, fresh fruit, a green salad, water and a seasonal dessert. Le Chic comes in at €85 ($62) and comes with all of the above, as well as Champagne, foie gras and macarons. Customers then select where and when they want their picnic delivered to. The company recommends giving them one to two days' notice but last-minute requests can be accommodated. The Tuileries gardens near the Louvre is one of Paris' perfect picnic spots in the spring time . For an additional fee, romantics can add a photo shoot, flowers or arrange a surprise set-up - perfect for those wanting to pop the question in the city of love. Each picnic includes cutlery and a cotton blanket  for sprawling on in the sunshine. And each one is delivered in a bright yellow Piaggio called Pepe. The picnics are the brainchild of American couple Patrick Johnson and Katia Kroupnik, who state on their website: 'Whether picnicking along the Canal St-Martin or under the Eiffel Tower, up in hilly Buttes Chaumont or in one of the countless parks that span Paris from the Bois de Boulogne to the Bois de Vincennes, it's easy to feel that Paris was made for le pique-nique. 'By partnering with artisan food and wine producers around France and offering free delivery to select locations across the city, Paris Picnic simplifies the process of planning and arranging a picnic.'\n"
     ]
    }
   ],
   "source": [
    "print(df['article'].values[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a67e09d-3e62-4033-a306-e30ccc975773",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 1.0\n",
      " Aggreement (labels only) :  0.78\n",
      " Aggreement (labels and sentences) :  0.72\n"
     ]
    }
   ],
   "source": [
    "''' Annotator agreement \n",
    "Same labels \n",
    "Same labels and sentences \n",
    "'''\n",
    "user_ids = list(set(df['user_id'].values))\n",
    "df_ann1 = df[df['user_id'] == user_ids[0]]\n",
    "df_ann2 = df[df['user_id'] == user_ids[1]]\n",
    "\n",
    "df_ann1_overlap = df_ann1[df_ann1['summary_uuid'].isin(df_ann2['summary_uuid'].values)]\n",
    "df_ann2_overlap = df_ann2[df_ann2['summary_uuid'].isin(df_ann1['summary_uuid'].values)]\n",
    "df_ann1_overlap = df_ann1_overlap.sort_values(['summary_uuid'])\n",
    "df_ann2_overlap = df_ann2_overlap.sort_values(['summary_uuid'])\n",
    "assert(list(df_ann1_overlap['summary_uuid'].values) == list(df_ann2_overlap['summary_uuid'].values))\n",
    "\n",
    "print('Progress', len(df_ann2_overlap)/100)\n",
    "label_aggr_condition = df_ann2_overlap['label_type'].values == df_ann1_overlap['label_type'].values\n",
    "sentence_aggr_condition = df_ann2_overlap['nonfactual_sentences'].values == df_ann1_overlap['nonfactual_sentences'].values\n",
    "\n",
    "print(' Aggreement (labels only) : ', \\\n",
    "      len(df_ann2_overlap[label_aggr_condition])/len(df_ann2_overlap))\n",
    "print(' Aggreement (labels and sentences) : ', \n",
    "     len(df_ann2_overlap[(label_aggr_condition) & (sentence_aggr_condition) ])/len(df_ann2_overlap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa243a-26a1-47fd-b9c8-dfafe55cf47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76079838-d1ce-48f0-81af-515de594b07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Error Statistics:\n",
    "Labeled as nonfactual --> absolute score (1/0)\n",
    "Labeld as nonfactual --> get overlapping sentences --> get score of #nonfact sentences/len(summary sents)\n",
    "Record relative scores and make a new df to get correlation\n",
    "'''\n",
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_error_stats(datatypes, qualified_annotators, agreement):\n",
    "    non_factual_dict = {\n",
    "                    'article': [],\n",
    "                    'summary': [],\n",
    "                    'model' : [],\n",
    "                    'data': [],\n",
    "                    'non_factual_sents': [], \n",
    "                    'label_type': [], \\\n",
    "                  'num_summary_sents' : []}\n",
    "    for datatype in datatypes:\n",
    "        df_data = df[df['summ_id'] == f'{datatype}_generic']\n",
    "        \n",
    "        for model in ['flant5', 'gpt3']:\n",
    "            df_model = df_data[df_data['system_id'] == model]\n",
    "\n",
    "            for unique_ids in list(set(df_model['summary_uuid'])):\n",
    "                \n",
    "                df_uuid = df_model[df_model['summary_uuid'] == unique_ids]\n",
    "                # print(df_uuid)\n",
    "                article = df_uuid['article'].values[0]\n",
    "                user_ids_uuid =   set(list(df_uuid['user_id'].values))\n",
    "                summary = df_uuid['summary'].values[0]\n",
    "                summ_sents =  list(nlp(summary).sents)\n",
    "                sentences_ann = list(df_uuid['nonfactual_sentences'].values)\n",
    "                    \n",
    "                annotated_sentences = []\n",
    "                for each_sent in sentences_ann:\n",
    "                    score = 0\n",
    "                    if each_sent:\n",
    "                        score = len(each_sent.split('<new_annotation>'))\n",
    "                    annotated_sentences.append(score)\n",
    "                \n",
    "                sentbased_score = np.mean(annotated_sentences)/len(summ_sents)\n",
    "                # print(sentbased_score, annotated_sentences)\n",
    "                non_factual_dict['article'] += [article]\n",
    "                non_factual_dict['summary'] += [summary]\n",
    "                non_factual_dict['model'] += [model]\n",
    "                non_factual_dict['data'] += [datatype]\n",
    "                non_factual_dict['non_factual_sents'] += [sentbased_score]\n",
    "                label_type = 1 if sentbased_score >0 else 0\n",
    "                non_factual_dict['label_type'] += [label_type]\n",
    "                non_factual_dict['num_summary_sents'] += [len(summ_sents)]\n",
    "                            \n",
    "    return pd.DataFrame(non_factual_dict)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "95d12521-4512-46a2-a361-caa36801b30d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>model</th>\n",
       "      <th>data</th>\n",
       "      <th>non_factual_sents</th>\n",
       "      <th>label_type</th>\n",
       "      <th>num_summary_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anaerobic oxidation of methane coupled to sul...</td>\n",
       "      <td>sr-aom community was enriched in a high-pressu...</td>\n",
       "      <td>flant5</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a great deal of effort and expense are being ...</td>\n",
       "      <td>a java based linkage disequilibrium plotter</td>\n",
       "      <td>flant5</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the renin-angiotensin system is a hormone sys...</td>\n",
       "      <td>a simple, efficient method for the production ...</td>\n",
       "      <td>flant5</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metabolomic profiling in general aims to iden...</td>\n",
       "      <td>We introduce the marvis tool, which implements...</td>\n",
       "      <td>flant5</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one of the critical considerations in cereal ...</td>\n",
       "      <td>rht genes and fusarium diseases in barley and ...</td>\n",
       "      <td>flant5</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phage display libraries consist of small anti...</td>\n",
       "      <td>Phage display libraries are used for antibody ...</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>quantitative real-time reverse transcription ...</td>\n",
       "      <td>The article discusses the importance of normal...</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>gene knock out technology is widely used to s...</td>\n",
       "      <td>The article discusses the challenges associate...</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>metabolomic profiling in general aims to iden...</td>\n",
       "      <td>The article discusses Marvis, a tool for metab...</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>the isolation of mycobacterium tuberculosis c...</td>\n",
       "      <td>Mycobacterium tuberculosis isolation from clin...</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              article   \n",
       "0    anaerobic oxidation of methane coupled to sul...  \\\n",
       "1    a great deal of effort and expense are being ...   \n",
       "2    the renin-angiotensin system is a hormone sys...   \n",
       "3    metabolomic profiling in general aims to iden...   \n",
       "4    one of the critical considerations in cereal ...   \n",
       "..                                                ...   \n",
       "95   phage display libraries consist of small anti...   \n",
       "96   quantitative real-time reverse transcription ...   \n",
       "97   gene knock out technology is widely used to s...   \n",
       "98   metabolomic profiling in general aims to iden...   \n",
       "99   the isolation of mycobacterium tuberculosis c...   \n",
       "\n",
       "                                              summary   model    data   \n",
       "0   sr-aom community was enriched in a high-pressu...  flant5  pubmed  \\\n",
       "1         a java based linkage disequilibrium plotter  flant5  pubmed   \n",
       "2   a simple, efficient method for the production ...  flant5  pubmed   \n",
       "3   We introduce the marvis tool, which implements...  flant5  pubmed   \n",
       "4   rht genes and fusarium diseases in barley and ...  flant5  pubmed   \n",
       "..                                                ...     ...     ...   \n",
       "95  Phage display libraries are used for antibody ...    gpt3  pubmed   \n",
       "96  The article discusses the importance of normal...    gpt3  pubmed   \n",
       "97  The article discusses the challenges associate...    gpt3  pubmed   \n",
       "98  The article discusses Marvis, a tool for metab...    gpt3  pubmed   \n",
       "99  Mycobacterium tuberculosis isolation from clin...    gpt3  pubmed   \n",
       "\n",
       "    non_factual_sents  label_type  num_summary_sents  \n",
       "0            0.333333           1                  3  \n",
       "1            0.000000           0                  1  \n",
       "2            0.000000           0                  1  \n",
       "3            0.000000           0                  1  \n",
       "4            1.000000           1                  1  \n",
       "..                ...         ...                ...  \n",
       "95           0.000000           0                  4  \n",
       "96           0.000000           0                  4  \n",
       "97           0.000000           0                  9  \n",
       "98           0.000000           0                  6  \n",
       "99           0.000000           0                  4  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatypes = ['xsum', 'cnndm']\n",
    "# datatypes = ['billsum']\n",
    "# datatypes = ['pubmed']\n",
    "qualified_annotators = qualified_annotators_news\n",
    "agreement = 2\n",
    "df_error = make_error_stats(datatypes, qualified_annotators, agreement)\n",
    "# df_error.to_csv(f'error_scores_{\"_\".join(datatypes)}.csv')\n",
    "df_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4db3893e-5204-43bd-930e-556c2f2f1c36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pubmed\n",
      "Model flant5\n",
      "Avg num sentences 2.28\n",
      "0.06115196078431372\n",
      "Model gpt3\n",
      "Avg num sentences 5.46\n",
      "0.028070818070818068\n"
     ]
    }
   ],
   "source": [
    "####### News\n",
    "for data in list(set(df_error['data'])):\n",
    "    print('Data' , data)\n",
    "    for model in list(set(df_error['model'])):\n",
    "        print('Model', model)\n",
    "        df_error_dm = df_error[(df_error['data'] == data) & (df_error['model'] == model)]\n",
    "        num_non_factual = list(df_error_dm['label_type'].values)\n",
    "        num_non_factual_sents = list(df_error_dm['non_factual_sents'].values)\n",
    "        num_summ_sents = list(df_error_dm['num_summary_sents'].values)\n",
    "        if len(df_error_dm):\n",
    "            print('Avg num sentences', np.mean(num_summ_sents))\n",
    "            # print(sum(num_non_factual)/len(df_error_dm))\n",
    "            print(np.mean(num_non_factual_sents))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "843d82b2-2d70-4a6e-a796-808c08fed4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model flant5\n",
      "Avg num sentences 1.0\n",
      "0.23\n",
      "Model gpt3\n",
      "Avg num sentences 4.84\n",
      "0.2008095238095238\n"
     ]
    }
   ],
   "source": [
    "for model in list(set(df_error['model'])):\n",
    "        print('Model', model)\n",
    "        df_error_dm = df_error[(df_error['model'] == model)]\n",
    "        num_non_factual = list(df_error_dm['label_type'].values)\n",
    "        num_non_factual_sents = list(df_error_dm['non_factual_sents'].values)\n",
    "        num_summ_sents = list(df_error_dm['num_summary_sents'].values)\n",
    "        if len(df_error_dm):\n",
    "            print('Avg num sentences', np.mean(num_summ_sents))\n",
    "            # print(sum(num_non_factual)/len(df_error_dm))\n",
    "            print(np.mean(num_non_factual_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca6c091c-ce37-4438-b2a9-40517d6ff819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data</th>\n",
       "      <th>non_factual_sents</th>\n",
       "      <th>label_type</th>\n",
       "      <th>num_summary_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, data, non_factual_sents, label_type, num_summary_sents]\n",
       "Index: []"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "401bc969-76c8-4408-a180-1e0a8c86b805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCorrelation of summary length with factuality score\\n\\n'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Correlation of summary length with factuality score\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6e2a0a80-1a6d-42da-b954-9be8021e9f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_error_stats(df, data, qualified_annotators, agreement = 2):\n",
    "    print('Fraction of factual errors')\n",
    "    df_data = df[df['summ_id'] == f'{data}_generic']\n",
    "    print(f'{data}_generic', len(df_data))\n",
    "    for model in ['flant5', 'gpt3']:\n",
    "        df_model = df_data[df_data['system_id'] == model]\n",
    "        print(len(df_model))\n",
    "        non_factual_num = []\n",
    "        filtered_ids = []\n",
    "        for unique_ids in list(set(df_model['summary_uuid'])):\n",
    "            df_uuid = df_model[df_model['summary_uuid'] == unique_ids]\n",
    "            user_ids_uuid =   set(list(df_uuid['user_id'].values))\n",
    "            if len(user_ids_uuid) >= agreement:\n",
    "                filtered_ids.append(unique_ids)\n",
    "                label_types = list(df_uuid['label_type'].values)\n",
    "\n",
    "                if len(set(label_types)) == 1:\n",
    "                    if label_types[0] == 'non_factual':\n",
    "                        non_factual_num.append(unique_ids)\n",
    "        print(model, len(non_factual_num)/len(filtered_ids), len(filtered_ids))\n",
    "            \n",
    "    \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c4a4f3f0-8050-4644-b7fd-e8e37c22c817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of factual errors\n",
      "xsum_generic 98\n",
      "49\n",
      "flant5 0.5 24\n",
      "49\n",
      "gpt3 0.4583333333333333 24\n",
      "Fraction of factual errors\n",
      "cnndm_generic 45\n",
      "22\n",
      "flant5 0.0 7\n",
      "23\n",
      "gpt3 0.2857142857142857 7\n"
     ]
    }
   ],
   "source": [
    "get_error_stats(df, 'xsum', qualified_annotators_news)\n",
    "get_error_stats(df, 'cnndm', qualified_annotators_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "648bbbe4-e9e9-4538-8748-947bb3956b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_dicts(df):\n",
    "    summary_annotations = {}\n",
    "    label_annotations = {}\n",
    "    sent_annotations = {}\n",
    "    all_summ_uuids = sorted(list(set(df['summary_uuid'].values)), key = len)\n",
    "\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        summ_uuid = row['summary_uuid']\n",
    "        label_type = row['label_type']\n",
    "        nonfact_sents = row['nonfactual_sentences']\n",
    "\n",
    "        if summ_uuid not in summary_annotations:\n",
    "            summary_annotations[summ_uuid] = {}\n",
    "\n",
    "        if user_id not in label_annotations:\n",
    "            label_annotations[user_id] = {'factual': [], 'non_factual': []}\n",
    "            sent_annotations[user_id] = [''] * len(all_summ_uuids)\n",
    "\n",
    "        if label_type not in summary_annotations[summ_uuid]:\n",
    "            summary_annotations[summ_uuid][label_type] = []\n",
    "\n",
    "        summary_annotations[summ_uuid][label_type].append((user_id, nonfact_sents))\n",
    "\n",
    "        label_annotations[user_id][label_type].append(summ_uuid)\n",
    "    #     print(summ_uuid, all_summ_uuids.index(summ_uuid))\n",
    "        sent_annotations[user_id][all_summ_uuids.index(summ_uuid)] = nonfact_sents\n",
    "    \n",
    "    return summary_annotations, label_annotations, sent_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b2062-d1f5-45f3-b66a-6b521bc22e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3cc85512-7468-4e5b-98ad-152c97fba026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_annotator_matrix_label(label_annotations):\n",
    "    all_ann_ids = sorted(list(label_annotations.keys()), key = len)\n",
    "    ann_matrix = {}\n",
    "    ann_matrix['annotators'] = all_ann_ids\n",
    "    for ann_id in all_ann_ids:\n",
    "        if ann_id not in ann_matrix:\n",
    "            ann_matrix[ann_id] = []\n",
    "        ann_id_len = len(label_annotations[ann_id]['non_factual']) \n",
    "        for idx, other_ann_id in enumerate(all_ann_ids):\n",
    "            num_factual_aggr = set(label_annotations[ann_id]['non_factual']).intersection(label_annotations[other_ann_id]['non_factual'])\n",
    "            # num_nonfactual_aggr = set(label_annotations[ann_id]['factual']).intersection(label_annotations[other_ann_id]['factual'])\n",
    "            ann_matrix[ann_id].append(len(num_factual_aggr)/ann_id_len)\n",
    "    return ann_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c9ff17b-dcb8-466c-81b5-2f6de27466c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_annotator_matrix_sent(sent_annotations):\n",
    "    all_ann_ids = sorted(list(label_annotations.keys()), key = len)\n",
    "    ann_matrix = {}\n",
    "    ann_matrix['annotators'] = all_ann_ids\n",
    "    for ann_id in all_ann_ids:\n",
    "        if ann_id not in ann_matrix:\n",
    "            ann_matrix[ann_id] = []\n",
    "            \n",
    "        # ann_sents = sent_annotations[ann_id]\n",
    "        \n",
    "        for other_ann_id in all_ann_ids:\n",
    "            other_ann_sent_counter = 0\n",
    "            # other_ann_sents = sent_annotations[ann_id]\n",
    "            # other_ann_sents = other_ann_sents.split('<new_annotation>')\n",
    "            other_ann_sent_counter = []\n",
    "            for idx, ann_sent in enumerate(sent_annotations[ann_id]):\n",
    "#                 print(idx)\n",
    "                other_ann_sent = sent_annotations[other_ann_id][idx]\n",
    "                ann_sents_idx = ann_sent.split('<new_annotation>')\n",
    "                other_ann_sents_idx = other_ann_sent.split('<new_annotation>')\n",
    "                \n",
    "                val = len(set(ann_sents_idx).intersection(other_ann_sents_idx))/len(ann_sents_idx)\n",
    "                val = round(val, 1)\n",
    "                # print(val)\n",
    "                # if ann_sent == other_ann_sent:\n",
    "                #     val = 1\n",
    "                # elif (ann_sent in other_ann_sent) or (other_ann_sent in ann_sent):\n",
    "                #     val = 0.5\n",
    "                # else:\n",
    "                #     val = 0\n",
    "                other_ann_sent_counter += [val]\n",
    "            ann_matrix[ann_id].append(np.mean(other_ann_sent_counter))\n",
    "    return ann_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cef79a25-ed80-4f13-bb97-1c849f19aba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_annotations, label_annotations, sent_annotations = map_dicts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "43b1326c-1016-426c-a58e-287fe282f088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# label_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1718f63-9947-4aef-b195-0c9ceb5d34d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotators</th>\n",
       "      <th>ann_japq</th>\n",
       "      <th>ann_tpfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ann_japq</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.675676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ann_tpfo</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotators  ann_japq  ann_tpfo\n",
       "0   ann_japq   1.00000  0.675676\n",
       "1   ann_tpfo   0.78125  1.000000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_matrix = compute_annotator_matrix_label(label_annotations)\n",
    "df_matrix = pd.DataFrame(ann_matrix)\n",
    "df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6d9e0c0-8057-48b6-9416-7c1ef57c95e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotators</th>\n",
       "      <th>ann_japq</th>\n",
       "      <th>ann_tpfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ann_japq</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.738272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ann_tpfo</td>\n",
       "      <td>0.749383</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotators  ann_japq  ann_tpfo\n",
       "0   ann_japq  1.000000  0.738272\n",
       "1   ann_tpfo  0.749383  1.000000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_matrix = compute_annotator_matrix_sent(sent_annotations)\n",
    "df_sent_matrix = pd.DataFrame(sent_matrix)\n",
    "df_sent_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "912371b6-424a-49e0-b8a2-67648cab0f08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_annotations['ann_krcnbm']['non_factual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109fdb8-68f2-4e75-8070-65f1bad5300a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
