{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a85169-e93a-4c91-9ec3-e0f9ac612d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import string \n",
    "import re\n",
    "\n",
    "import json \n",
    "\n",
    "import pandas as pd \n",
    "import sqlite3\n",
    "import spacy\n",
    "import numpy as np\n",
    "from rouge import Rouge \n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d034d95e-87d7-441b-985d-d0ae2016fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations(task):\n",
    "    df_ann = pd.read_csv(f'/home/ramprasad.sa/factual_annotation_llm_summaries/annotations/{task}_annotations.csv', keep_default_na=False)\n",
    "    df_ann_scores = pd.read_csv(f'/home/ramprasad.sa/factual_annotation_llm_summaries/annotations/{task}_annotation_scores.csv', keep_default_na=False)\n",
    "    return df_ann, df_ann_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d60fdf9-256a-41af-a7e7-fc6d944e9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ann_news, df_ann_scores_news = read_annotations('news')\n",
    "df_ann_billsum, df_ann_scores_billsum = read_annotations('billsum')\n",
    "df_ann_pubmed, df_ann_scores_pubmed = read_annotations('pubmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1dedc27-4fc7-41bb-a4d6-0ace7cf2cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from QAFactEval.qafacteval import QAFactEval\n",
    "# from questeval.questeval_metric import QuestEval\n",
    "from summac.model_summac import SummaCZS, SummaCConv\n",
    "\n",
    "def get_questeval_score(articles, summaries):\n",
    "    questeval = QuestEval(no_cuda=False)\n",
    "    score = questeval.corpus_questeval(\n",
    "    hypothesis=summaries, \n",
    "    sources=articles\n",
    "    )\n",
    "    return score['ex_level_scores']\n",
    "\n",
    "def get_summac_score(articles, summaries):\n",
    "    device = 'cuda'\n",
    "    model_zs = SummaCZS(granularity=\"sentence\", model_name=\"vitc\", device=device) # If you have a GPU: switch to: device=\"cuda\"\n",
    "    model_conv = SummaCConv(models=[\"vitc\"], bins='percentile', granularity=\"sentence\", nli_labels=\"e\", device=device, start_file=\"default\", agg=\"mean\")\n",
    "    score_zs1 = model_zs.score(articles, summaries)\n",
    "    score_conv1 = model_conv.score(articles, summaries)\n",
    "    return score_zs1['scores'], score_conv1['scores']\n",
    "\n",
    "def get_qafacteval_score(articles, summaries):\n",
    "    kwargs = {\"cuda_device\": 0, \"use_lerc_quip\": True, \\\n",
    "        \"verbose\": True, \"generation_batch_size\": 32, \\\n",
    "        \"answering_batch_size\": 32, \"lerc_batch_size\": 8}\n",
    "\n",
    "    model_folder = \"/home/ramprasad.sa/factual_annotation_llm_summaries/analysis/notebook/QAFactEval/models\" # path to models downloaded with download_models.sh\n",
    "    metric = QAFactEval(\n",
    "        lerc_quip_path=f\"{model_folder}/quip-512-mocha\",\n",
    "        generation_model_path=f\"{model_folder}/generation/model.tar.gz\",\n",
    "        answering_model_dir=f\"{model_folder}/answering\",\n",
    "        lerc_model_path=f\"{model_folder}/lerc/model.tar.gz\",\n",
    "        lerc_pretrained_model_path=f\"{model_folder}/lerc/pretraining.tar.gz\",\n",
    "        **kwargs\n",
    "    )\n",
    "    qafacteval_scores = []\n",
    "    docs = []\n",
    "    summs = []\n",
    "    for art, summ in list(zip(articles, summaries)):\n",
    "        docs.append(art)\n",
    "        summs.append([summ])\n",
    "    results = metric.score_batch_qafacteval(docs, summs, return_qa_pairs=True)\n",
    "    results = [each[0]['qa-eval']['lerc_quip'] for each in results]\n",
    "    # results = metric.score_batch_qafacteval([art], [[summ]], return_qa_pairs=True)\n",
    "    # score = results[0][0]['qa-eval']['lerc_quip']\n",
    "    # qafacteval_scores.append(score)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ec275b-8fb3-451d-a58c-aa9584f161bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# from bart_score import BARTScorer\n",
    "def make_summac(df_errors):\n",
    "    summaczs, summacc = get_summac_score(list(df_errors['article'].values), list(df_errors['summary'].values))\n",
    "    df_errors['SummaC_Conv'] = summacc\n",
    "    df_errors['SummaC_ZS'] = summaczs\n",
    "    y_scores = [1 - each for each in df_errors['score']]\n",
    "    print(stats.spearmanr(summaczs, y_scores), stats.spearmanr(summacc, y_scores))\n",
    "    print(stats.pearsonr(summaczs, y_scores), stats.pearsonr(summacc, y_scores))\n",
    "    return df_errors\n",
    "\n",
    "def make_questeval(df_errors):\n",
    "    questeval_score = get_questeval_score(list(df_errors['article'].values), list(df_errors['summary'].values))\n",
    "    df_errors['QuestEval'] = questeval_score\n",
    "    y_scores = [1 - each for each in df_errors['score']]\n",
    "    print(stats.spearmanr(questeval_score, y_scores))\n",
    "    print(stats.pearsonr(questeval_score, y_scores))\n",
    "    return df_errors\n",
    "\n",
    "def make_qafacteval(df_errors):\n",
    "    qafacteval_scores = get_qafacteval_score(list(df_errors['article'].values), list(df_errors['summary'].values))\n",
    "    df_errors['QAFactEval'] = qafacteval_scores\n",
    "    y_scores = [1 - each for each in df_errors['score']]\n",
    "    print(stats.spearmanr(qafacteval_scores, y_scores))\n",
    "    print(stats.pearsonr(qafacteval_scores, y_scores))\n",
    "    return df_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b5214b5-02fb-4173-b64f-aa679c307cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ann_scores_news = make_qafacteval(df_ann_scores_news)\n",
    "# df_ann_scores_billsum = make_qafacteval(df_ann_scores_billsum)\n",
    "# df_ann_scores_pubmed = make_qafacteval(df_ann_scores_pubmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adfbc2f2-fab2-484c-a4d7-f788ec82757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ann_scores_news = make_questeval(df_ann_scores_news)\n",
    "# df_ann_scores_billsum = make_questeval(df_ann_scores_billsum)\n",
    "# df_ann_scores_pubmed = make_questeval(df_ann_scores_pubmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255712d1-19ed-4108-9c47-b47a40f84e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "SignificanceResult(statistic=0.1147427565379258, pvalue=0.25563834223345394) SignificanceResult(statistic=0.06192422559742772, pvalue=0.5405082149295017)\n",
      "PearsonRResult(statistic=0.007647056425181448, pvalue=0.9398087557446342) PearsonRResult(statistic=-0.04615249113414532, pvalue=0.6484147889125043)\n"
     ]
    }
   ],
   "source": [
    "# df_ann_scores_news = make_summac(df_ann_scores_news)\n",
    "# df_ann_scores_billsum = make_summac(df_ann_scores_billsum)\n",
    "df_ann_scores_pubmed = make_summac(df_ann_scores_pubmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0213986-72b8-446a-a53f-9eb9419e7480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facteval",
   "language": "python",
   "name": "facteval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
