{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f11388-3f38-4fce-9aea-36b57230794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import string \n",
    "import re\n",
    "\n",
    "import json \n",
    "\n",
    "import pandas as pd \n",
    "import sqlite3\n",
    "import spacy\n",
    "import numpy as np\n",
    "from rouge import Rouge \n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ccf2c9d-9de8-40f2-882c-59a2be661f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "qualified_annotators_dict = {'pubmed': ['ann_jclvzw', 'ann_eftpco'],\n",
    "                            'news': ['ann_japq', 'ann_tpfo'],\n",
    "                            'billsum': ['ann_krcnbm', 'ann_hguilf']}\n",
    "\n",
    "\n",
    "def connect_to_db(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    return conn, c \n",
    "\n",
    "def read_annotations(db_path, task):\n",
    "    # /human_annotations_factuality/XSUM_CNN\n",
    "    parent_path = '/home/ramprasad.sa'\n",
    "    db_path_round1 = f'{parent_path}/{db_path}/set1/annotated/{task}_summaries_set1_final.db'\n",
    "    print(db_path_round1)\n",
    "    conn, c = connect_to_db(db_path_round1)\n",
    "    df_annotations_round1  = pd.read_sql('SELECT * from label', conn)\n",
    "\n",
    "    db_path_round2 = f'{parent_path}/{db_path}/set1_round2/annotated/{task}_nonfactual_annotated_generated_summaries_fin.db'\n",
    "    conn, c = connect_to_db(db_path_round2)\n",
    "    df_annotations_round2 = pd.read_sql('SELECT * from error_label', conn)\n",
    "    return df_annotations_round1, df_annotations_round2\n",
    "    \n",
    "    \n",
    "def remove_duplicates(df, task):\n",
    "    \n",
    "    qualified_annotators_dict = {'pubmed': ['ann_jclvzw', 'ann_eftpco'],\n",
    "                            'news': ['ann_japq', 'ann_tpfo'],\n",
    "                            'billsum': ['ann_krcnbm', 'ann_hguilf']}\n",
    "    qualified_annotators = qualified_annotators_dict[task]\n",
    "    processed_rows = []\n",
    "    \n",
    "    for annotator in qualified_annotators:\n",
    "        df_qualified_annotators = df[df['user_id'] == annotator]\n",
    "        # print(df_qualified_annotators)\n",
    "        for each_id in list(set(df_qualified_annotators['summary_uuid'].values)):\n",
    "            df_uid = df_qualified_annotators[df_qualified_annotators['summary_uuid'] == each_id]\n",
    "            if len(df_uid) == 1:\n",
    "                row_append = df_uid.iloc[[0]]\n",
    "            else:\n",
    "                row_append = df_uid.iloc[[-1]]\n",
    "            processed_rows.append(row_append)\n",
    "            \n",
    "    df_processed = pd.concat(processed_rows)\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "def refine_df(df_annotations, task):\n",
    "    \n",
    "    df_annotations = remove_duplicates(df_annotations, task)\n",
    "    return df_annotations\n",
    "\n",
    "def test_annotation_rounds(df_annotations_round1, df_annotations_round2, task):\n",
    "    \n",
    "    \n",
    "    for qual_ann in qualified_annotators_dict[task]:\n",
    "        df_ann_r1 = df_annotations_round1[df_annotations_round1['user_id'] == qual_ann]\n",
    "        df_ann_r1_sents = df_ann_r1[df_ann_r1['label_type'] == 'non_factual']['nonfactual_sentences'].values\n",
    "        df_ann_r1_sents = [len(each.split('<new_annotation>')) for each in df_ann_r1_sents ]\n",
    "        \n",
    "        df_ann_r2 = df_annotations_round2[df_annotations_round2['user_id'] == qual_ann]\n",
    "        assert sum(df_ann_r1_sents) ==  len(df_ann_r2)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def make_sentence_error_maps(task_annotators,  df_round2_article_summ):\n",
    "    sentence_error_category_map = {}\n",
    "    if not len(df_round2_article_summ):\n",
    "            sentence_error_category_map[None] = {0: (None, None, None, None), 1: (None, None, None, None)}\n",
    "            \n",
    "    else:\n",
    "        for annotator in task_annotators:\n",
    "            df_round2_article_summ_annotator = df_round2_article_summ[df_round2_article_summ['user_id'] == annotator]\n",
    "            for error_idx, error_row in df_round2_article_summ_annotator.iterrows():\n",
    "                    user_id = task_annotators.index(error_row['user_id'])\n",
    "                    nonfactual_sentence = error_row['nonfactual_sentence']\n",
    "                    inaccuracy_severity = error_row['inaccuracy_severity']\n",
    "                    error_type = error_row['error_type']\n",
    "                    error_factuality = error_row['error_factuality'] \n",
    "                    error_factuality = error_factuality if error_factuality else None\n",
    "                    comments = error_row['comments']\n",
    "                    comments = comments if comments else None\n",
    "\n",
    "                    if nonfactual_sentence not in sentence_error_category_map:\n",
    "                        sentence_error_category_map[nonfactual_sentence] = {0: (None, None, None, None), 1: (None, None, None, None)}\n",
    "                    sentence_error_category_map[nonfactual_sentence][user_id] = (inaccuracy_severity, error_type, error_factuality, comments)\n",
    "    return sentence_error_category_map\n",
    "    \n",
    "\n",
    "\n",
    "def rectified_annotations(df_aggr, ann_idx):\n",
    "    \n",
    "    label_types_ann = list(df_aggr[f'label_type_ann{ann_idx}'].values)\n",
    "    inaccuracy_severity_ann = list(df_aggr[f'inaccuracy_severity_ann{ann_idx}'].values)\n",
    "    error_type_ann = list(df_aggr[f'error_type_ann{ann_idx}'].values)\n",
    "    error_factuality_ann = list(df_aggr[f'error_factuality_ann{ann_idx}'].values)\n",
    "    \n",
    "    for idx, row in df_aggr.iterrows():\n",
    "        comment = row[f'comments_ann{ann_idx}']\n",
    "        error_type = row[f'error_type_ann{ann_idx}']\n",
    "        if comment != None :\n",
    "            comment = comment.lower()\n",
    "            if 'accurate' in comment or 'factual' in comment or 'this is correct' in comment or 'misidentified' in comment:\n",
    "                # print(comment, idx, label_types_ann[idx])\n",
    "                label_types_ann[idx] =  'factual'\n",
    "                # print(label_types_ann[idx])\n",
    "                inaccuracy_severity_ann[idx] = None\n",
    "                error_type_ann[idx] = None\n",
    "                error_factuality_ann[idx] = None\n",
    "                \n",
    "    df_aggr[f'label_type_ann{ann_idx}'] = label_types_ann\n",
    "    df_aggr[f'inaccuracy_severity_ann{ann_idx}'] = inaccuracy_severity_ann\n",
    "    df_aggr[f'error_type_ann{ann_idx}'] = error_type_ann\n",
    "    df_aggr[f'error_factuality_ann{ann_idx}'] = error_factuality_ann\n",
    "    return df_aggr\n",
    "\n",
    "def make_aggr_df(df_annotations_round1, df_annotations_round2, task):\n",
    "    \n",
    "    df_processed_dict = {\n",
    "    'summary_uuid': [],\n",
    "    'summary': [],\n",
    "    'article': [],\n",
    "    'summ_id': [],\n",
    "    'system_id': [],\n",
    "    'nonfactual_sentence': [],\n",
    "    'label_type_ann1': [],\n",
    "    'label_type_ann2': [],\n",
    "    'inaccuracy_severity_ann1': [],\n",
    "    'inaccuracy_severity_ann2': [],\n",
    "    'error_type_ann1': [],\n",
    "    'error_type_ann2': [],  \n",
    "    'error_factuality_ann1': [],\n",
    "    'error_factuality_ann2': [],\n",
    "    'comments_ann1': [],\n",
    "    'comments_ann2': [],\n",
    "\n",
    "    }\n",
    "    \n",
    "    unique_articles = list(set(df_annotations_round1['article'].values))\n",
    "    task_annotators = qualified_annotators_dict[task]\n",
    "\n",
    "    for article in unique_articles:\n",
    "        df_round1_article = df_annotations_round1[df_annotations_round1['article'] == article]\n",
    "        df_round2_article = df_annotations_round2[df_annotations_round2['article'] == article]\n",
    "\n",
    "        article_summaries = list(set(df_round1_article['summary'].values))\n",
    "\n",
    "        for article_summary in article_summaries:\n",
    "\n",
    "            df_round1_article_summ = df_round1_article[df_round1_article['summary'] == article_summary]\n",
    "            df_round2_article_summ = df_round2_article[df_round2_article['summary'] == article_summary]\n",
    "            if len(df_round1_article_summ) <2 :\n",
    "                print(article_summary)\n",
    "            else:\n",
    "                label_type_annotations = []\n",
    "                for annotator in task_annotators:\n",
    "                    df_round1_article_summ_annotator = df_round1_article_summ[df_round1_article_summ['user_id'] == annotator]\n",
    "\n",
    "                    label_type_annotations.append(df_round1_article_summ_annotator['label_type'].values[0])\n",
    "\n",
    "                sentence_error_category_map = make_sentence_error_maps(task_annotators,  df_round2_article_summ)\n",
    "\n",
    "\n",
    "\n",
    "                for nonfactual_sentence, nonfactual_sentence_annotations in sentence_error_category_map.items():\n",
    "                    summary_uuid = df_round1_article_summ['summary_uuid'].values[0]\n",
    "                    summ_id = df_round1_article_summ['summ_id'].values[0]\n",
    "                    system_id = df_round1_article_summ['system_id'].values[0]\n",
    "                    # pid = list(set(df_round2_article_summ['pid'].values))\n",
    "                    summary = df_round1_article_summ['summary'].values[0]\n",
    "\n",
    "                    df_processed_dict['summary_uuid'].append(summary_uuid)\n",
    "                    df_processed_dict['summ_id'].append(summ_id)\n",
    "                    system_id_append = 'GPT 3.5' if system_id == 'gpt3' else 'Flan T5'\n",
    "                    df_processed_dict['system_id'].append(system_id_append)\n",
    "                    # df_processed_dict['pid'].append(pid)\n",
    "                    # df_processed_dict['pid_2'].append(pid)\n",
    "                    df_processed_dict['summary'].append(summary)\n",
    "                    df_processed_dict['article'].append(article)\n",
    "\n",
    "                    df_processed_dict['nonfactual_sentence'].append(nonfactual_sentence)\n",
    "                    for lid, label_annotations in enumerate(label_type_annotations):\n",
    "                        df_processed_dict[f'label_type_ann{lid+1}'] += [label_annotations]\n",
    "\n",
    "                    # ann_vals = {\n",
    "                    for ann_id, ann_vals in nonfactual_sentence_annotations.items():\n",
    "                        ann_id = ann_id + 1\n",
    "                        df_processed_dict[f'inaccuracy_severity_ann{ann_id}'].append(ann_vals[0])\n",
    "                        df_processed_dict[f'error_type_ann{ann_id}'].append(ann_vals[1])\n",
    "                        df_processed_dict[f'error_factuality_ann{ann_id}'].append(ann_vals[2])\n",
    "                        df_processed_dict[f'comments_ann{ann_id}'].append(ann_vals[3])\n",
    "                        \n",
    "    return pd.DataFrame(df_processed_dict)\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "def compute_score(df_aggr, task):\n",
    "\n",
    "    unique_ids = list(set(df_aggr['summary_uuid']))\n",
    "    df_errors_scores = {'summary_uuid': [], 'score': [], 'model': [], 'task': [], 'summary': [], 'article': []}\n",
    "    for model in ['Flan T5', 'GPT 3.5']:\n",
    "        \n",
    "        df_model = df_aggr[df_aggr['system_id'] == model]\n",
    "        unique_ids = list(set(df_model['summary_uuid']))\n",
    "        print(len(unique_ids))\n",
    "        model_sentbased_scores = []\n",
    "        for uuid in unique_ids:\n",
    "\n",
    "            df_uuid = df_model[df_model['summary_uuid'] == uuid]\n",
    "            # print(df_uuid)\n",
    "            # summary = df_uuid['summary'].values[0]\n",
    "            \n",
    "            ann1_scores = []\n",
    "            ann2_scores = []\n",
    "            for idx, row in df_uuid.iterrows():\n",
    "                summary = row['summary']\n",
    "                summ_sents =  list(nlp(summary).sents)\n",
    "                nonfactual_sentence = row['nonfactual_sentence']\n",
    "                label_type_ann1 = row['inaccuracy_severity_ann1']\n",
    "                label_type_ann2 = row['inaccuracy_severity_ann2']\n",
    "                ann1_score = 0\n",
    "                ann2_score = 0\n",
    "                \n",
    "                if label_type_ann1 != None:\n",
    "                    ann1_score = 1\n",
    "                if label_type_ann2 != None:\n",
    "                    ann2_score = 1\n",
    "\n",
    "                ann1_scores.append(ann1_score)\n",
    "                ann2_scores.append(ann2_score)\n",
    "\n",
    "            ann1_score_uuid = sum(ann1_scores)\n",
    "            ann2_score_uuid = sum(ann2_scores)\n",
    "\n",
    "            sentbased_score = np.mean([ann1_score_uuid, ann2_score_uuid])/len(summ_sents)\n",
    "            # print([ann1_score_uuid, ann2_score_uuid], row)\n",
    "            model_sentbased_scores.append(sentbased_score)\n",
    "            df_errors_scores['summary_uuid'].append(uuid) \n",
    "            df_errors_scores['score'].append(sentbased_score)\n",
    "            # model_append = 'GPT-3.5' if model == 'gpt3' else 'Flan-T5-XL'\n",
    "            df_errors_scores['model'].append(model)\n",
    "            df_errors_scores['task'].append(task)\n",
    "            df_errors_scores['summary'].append(summary)\n",
    "            df_errors_scores['article'].append(row['article'])\n",
    "        print(model.upper(), np.mean(model_sentbased_scores))\n",
    "        \n",
    "    return pd.DataFrame(df_errors_scores)\n",
    "    \n",
    "    \n",
    "def run_scoring(db_path, task):\n",
    "    df_annotations_round1, df_annotations_round2 = read_annotations(db_path, task)\n",
    "    print(len(df_annotations_round1), len(df_annotations_round2))\n",
    "    df_annotations_round2 = refine_df(df_annotations_round2, task)\n",
    "    df_annotations_round1 = remove_duplicates(df_annotations_round1, task)\n",
    "    print('removed duplicates...', len(df_annotations_round1), len(df_annotations_round2))\n",
    "\n",
    "    test_annotation_rounds(df_annotations_round1, df_annotations_round2, task)\n",
    "    df_aggr = make_aggr_df(df_annotations_round1, df_annotations_round2, task)\n",
    "    df_aggr = rectified_annotations(df_aggr, ann_idx = 1)\n",
    "    df_aggr = rectified_annotations(df_aggr, ann_idx = 2)\n",
    "    df_errors = compute_score(df_aggr, task)\n",
    "    df_aggr.to_csv(f'/home/ramprasad.sa/factual_annotation_llm_summaries/annotations/{task}_annotations.csv')\n",
    "    df_errors.to_csv(f'/home/ramprasad.sa/factual_annotation_llm_summaries/annotations/{task}_annotation_scores.csv')\n",
    "    return df_errors, df_aggr   \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "656bf5cf-2a01-4618-a20d-041d44166eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ramprasad.sa//human_annotations_factuality/XSUM_CNN/set1/annotated/news_summaries_set1_final.db\n",
      "219 118\n",
      "removed duplicates... 201 114\n",
      "Delhi Dynamos has expressed an interest in signing Leicester City's midfielder, Esteban Cambiasso, for India's I-League. Nigel Pearson desires to keep the 34-year-old, who signed a one-year deal with Leicester last summer, for another season, however, this is dependent on Leicester's status in the Premier League. Leicester is currently in 18th place in the Premier League and faces a critical relegation encounter against Burnley at Turf Moor on Saturday.\n",
      "50\n",
      "FLAN T5 0.27\n",
      "50\n",
      "GPT 3.5 0.19126623376623375\n",
      "/home/ramprasad.sa//human_annotations_factuality/billsum/set1/annotated/billsum_summaries_set1_final.db\n",
      "233 123\n",
      "removed duplicates... 200 117\n",
      "50\n",
      "FLAN T5 0.19\n",
      "50\n",
      "GPT 3.5 0.15592857142857144\n",
      "/home/ramprasad.sa//human_annotations_factuality/pubmed/set1/annotated/pubmed_summaries_set1_final.db\n",
      "209 39\n",
      "removed duplicates... 200 36\n",
      "50\n",
      "FLAN T5 0.05336601307189543\n",
      "50\n",
      "GPT 3.5 0.02172161172161172\n"
     ]
    }
   ],
   "source": [
    "db_path_news = '/human_annotations_factuality/XSUM_CNN'\n",
    "task = 'news'\n",
    "df_errors_news, df_aggr_news = run_scoring(db_path_news, 'news')\n",
    "\n",
    "db_path_billsum = '/human_annotations_factuality/billsum'\n",
    "task = 'billsum'\n",
    "df_errors_billsum, df_aggr_billsum = run_scoring(db_path_billsum, task)\n",
    "\n",
    "db_path_pubmed = '/human_annotations_factuality/pubmed'\n",
    "task = 'pubmed'\n",
    "df_errors_pubmed, df_aggr_pubmed= run_scoring(db_path_pubmed, task)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00aa0d85-a8e3-4087-8475-27a4b877a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def cohen_kappa_scores(df):\n",
    "    ann1_label = list(df['label_type_ann1'].values)\n",
    "    ann2_label = list(df['label_type_ann2'].values)\n",
    "    print('Summary kappa', cohen_kappa_score(ann1_label, ann2_label))\n",
    "\n",
    "    error_type_ann1 = list(df['error_type_ann1'].values)\n",
    "    error_type_ann1_sent = ['error' if each else 'no_error' for each in error_type_ann1]\n",
    "    error_type_ann1_category = [each if each else 'no_error'  for each in error_type_ann1 ]\n",
    "    \n",
    "    error_type_ann2 = list(df['error_type_ann2'].values)\n",
    "    error_type_ann2_sent = ['error' if each else 'no_error' for each in error_type_ann2]\n",
    "    error_type_ann2_category = [each if each else 'no_error'  for each in error_type_ann2 ]\n",
    "    print(error_type_ann1_sent.count('error'), error_type_ann2_sent.count('error'))\n",
    "    print('Sentence kappa', cohen_kappa_score(error_type_ann1_sent, error_type_ann2_sent), len(error_type_ann1_sent), len(error_type_ann2_sent))\n",
    "    print('Category kappa', cohen_kappa_score(error_type_ann1_category, error_type_ann2_category))\n",
    "    \n",
    "def agreement_statistics(df):\n",
    "\n",
    "    agreement_sent_ids = []\n",
    "    agreement_summ_ids = []\n",
    "    agreement_categ_ids = []\n",
    "    both_categ_ids = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['label_type_ann1'] == row['label_type_ann2']:\n",
    "            agreement_summ_ids.append(row['summary_uuid'])\n",
    "        if (row['error_type_ann1'] and row['error_type_ann2']) or ( (not row['error_type_ann1']) and (not row['error_type_ann2'])):\n",
    "            agreement_sent_ids.append(row['summary_uuid'])\n",
    "        if row['error_type_ann1'] == row['error_type_ann2']:\n",
    "            # both_categ_ids.append(row['summary_uuid'])\n",
    "        #     if row['error_type_ann1'] == row['error_type_ann2']:\n",
    "            agreement_categ_ids.append(row['summary_uuid'])\n",
    "\n",
    "    \n",
    "    # print(cohen_kappa_score(ann1_summary_labels, ann2_summary_labels))\n",
    "    print('Annotator agreement summary level', \\\n",
    "          len(set(agreement_summ_ids))/len(set(df['summary_uuid'].values)), len(set(df['summary_uuid'].values)), \n",
    "        )\n",
    "    print('Annotator agreement sentence level', \n",
    "    len(agreement_sent_ids)/len(df))\n",
    "\n",
    "    print('Annotator agreement error categories level', \n",
    "    len(agreement_categ_ids)/len(df))\n",
    "\n",
    "    cohen_kappa_scores(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1570bc26-7fd7-46e9-a624-8a503ebe957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator agreement summary level 0.82 100\n",
      "Annotator agreement sentence level 0.7603305785123967\n",
      "Annotator agreement error categories level 0.6115702479338843\n",
      "Summary kappa 0.5835054385240259\n",
      "44 55\n",
      "Sentence kappa 0.5084745762711864 121 121\n",
      "Category kappa 0.3245040978738567\n"
     ]
    }
   ],
   "source": [
    "agreement_statistics(df_aggr_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdd32e52-ccbf-4209-af0e-5bbc78c60787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator agreement summary level 0.75 100\n",
      "Annotator agreement sentence level 0.5214285714285715\n",
      "Annotator agreement error categories level 0.5\n",
      "Summary kappa 0.4484848484848485\n",
      "38 57\n",
      "Sentence kappa -0.045941123996431577 140 140\n",
      "Category kappa 0.029126213592232886\n"
     ]
    }
   ],
   "source": [
    "agreement_statistics(df_aggr_billsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c16b8880-409f-4370-8ef7-224791c20f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator agreement summary level 0.85 100\n",
      "Annotator agreement sentence level 0.8415841584158416\n",
      "Annotator agreement error categories level 0.8217821782178217\n",
      "Summary kappa 0.14587737843551796\n",
      "Sentence kappa 0.14587737843551796 101 101\n",
      "Category kappa 0.07055214723926373\n"
     ]
    }
   ],
   "source": [
    "agreement_statistics(df_aggr_pubmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e64dd7-ccd1-4361-bc90-d94a68f6e158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
